{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHQ7s3ES8EDPHTvpCLjfh4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DayvisonGabriel/My_First_IAProject/blob/main/Teste_com_Modelo_de_IA_germini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalando a aplicação do google"
      ],
      "metadata": {
        "id": "LW_X_9Rv9rm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "CfyQd-xs9mwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregando a API KEY e o modelo generativo"
      ],
      "metadata": {
        "id": "3HHI3yDn9kWx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sWDhcuZagbqq"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import the Python SDK\n",
        "import google.generativeai as genai\n",
        "\n",
        "from google.colab import userdata",
        "api_key = userdata.get('secret_key')",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel('gemini-pro')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Corpo da logica do modelo criado agora"
      ],
      "metadata": {
        "id": "IePIBPJF90P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persona= input('Me diga como devo ser:')\n",
        "resposta = input('Faça sua pergunta!')\n",
        "conversa = 'ainda não conversamos'\n",
        "contexto = ''\n",
        "while resposta != \"\":\n",
        "  response = model.generate_content(f'sua diretriz é: {persona} e leve em consideração que já conversamos isso: {contexto} após isso entenda o comentario a seguir e continue a conversa dando a proxima resposta a questão, comentario:{resposta}')\n",
        "  print(response.text)\n",
        "\n",
        "  # CRIANDO UMA FORMA DE SALVAR E REUTILIZAR O HISTORICO, ELES FAZ DEMORAR MAIS O PROCESSAMENTO, POIS DEIXA O PROMPT MAIOR CADA VEZ QUE A CONVERSA AUMENTA.\n",
        "  conversa = model.generate_content(f'junte o que foi conversado anteriormente: {contexto} com minha nova pergunta:{resposta}. e sua nova resposta{response.text}, transcreva toda a conversa')\n",
        "  contexto = conversa.text\n",
        "  #print(conversa.text)\n",
        "  resposta = input('responda:')\n",
        "\n",
        "else:\n",
        "  print('Programação encerrada')"
      ],
      "metadata": {
        "id": "hsZoeh-0gn_E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
